# Pipeline de Datos de Criptomonedas

Un pipeline completo para la obtenciÃ³n, almacenamiento y anÃ¡lisis de datos de precios de criptomonedas.

## DescripciÃ³n General del Proyecto

Este proyecto proporciona un flujo de trabajo completo para:
1. Obtener datos histÃ³ricos de criptomonedas desde la API de CoinGecko
2. Almacenar datos en una base de datos PostgreSQL
3. Analizar tendencias y patrones de precios
4. Construir y evaluar modelos predictivos

## Estructura del Proyecto

```
crypto-data-pipeline/
â”œâ”€â”€ crypto_app/                  # MÃ³dulos CLI y conexiÃ³n a API/DB
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli.py                   # Punto de entrada con comandos: get-history, bulk-process, --store-db
â”‚   â”œâ”€â”€ coingecko_client.py      # LÃ³gica HTTP / anÃ¡lisis de JSON
â”‚   â””â”€â”€ db.py                    # ConexiÃ³n SQLAlchemy, inserciones y upserts mensuales
â”‚   â””â”€â”€ daily_fetch.py           # Script para ejecuciÃ³n diaria programada
â”‚
â”œâ”€â”€ sql/                         # Consultas SQL
â”‚   â”œâ”€â”€ create_tables.sql        # CreaciÃ³n de tablas DDL
â”‚   â””â”€â”€ analysis_queries.sql     # Consultas DQL para promedios mensuales y anÃ¡lisis de recuperaciÃ³n
â”‚
â”œâ”€â”€ analysis/                    # Scripts de anÃ¡lisis en Python
â”‚   â”œâ”€â”€ feature_engineering.py   # GeneraciÃ³n de caracterÃ­sticas + exportaciÃ³n de CSV/grÃ¡ficos
â”‚   â””â”€â”€ regression.py            # Entrenamiento/evaluaciÃ³n de modelos
â”‚
â”œâ”€â”€ notebooks/                   # Notebooks Jupyter para exploraciÃ³n interactiva
â”‚   â””â”€â”€ coin_analysis.ipynb
â”‚
â”œâ”€â”€ data/                        # Directorio de salida (JSONs, imÃ¡genes, CSVs)
â”‚
â”œâ”€â”€ logs/                        # Archivos de registro de la aplicaciÃ³n
â”‚
â”œâ”€â”€ Dockerfile                   # Imagen Python de la aplicaciÃ³n
â”œâ”€â”€ docker-compose.yml           # Orquesta la app + PostgreSQL con montaje de datos
â”œâ”€â”€ requirements.txt             # Dependencias del proyecto
â”œâ”€â”€ .gitignore                   # Reglas de ignorar en Git
â””â”€â”€ README.md                    # DocumentaciÃ³n del proyecto
```

## InstalaciÃ³n usando Docker 

1. AsegÃºrate de tener Docker y Docker Compose instalados en tu sistema.

2. Clona el repositorio:
   ```
   git clone https://github.com/facundofernandezmiguez/crypto-data-pipeline.git
   cd crypto-data-pipeline
   ```

3. Inicia los servicios:
   ```
   docker-compose up -d
   ```

Esto iniciarÃ¡ tanto la aplicaciÃ³n Python como la base de datos PostgreSQL.

## 1. Getting crypto token data ğŸ’¸

### Interfaz de LÃ­nea de Comandos

La CLI proporciona varios comandos para la obtenciÃ³n y procesamiento de datos:

#### Obtener una fecha especÃ­fica.

```
python -m crypto_app.cli get-history --coin bitcoin --date 2025-01-01
```

Si tambiÃ©n quieres almacenar los datos en la base de datos:

```
python -m crypto_app.cli get-history --coin bitcoin --date 2025-01-01 --store-db
```

#### Procesamiento diario automatizado
La aplicaciÃ³n incluye funcionalidad para ejecutarse automÃ¡ticamente cada dÃ­a a las 3:00 AM y obtener datos para bitcoin, ethereum y cardano.

Para ejecutar la obtenciÃ³n diaria de datos para bitcoin, ethereum y cardano:

```
python -m crypto_app.daily_fetch
```

Para ver las instrucciones de configuraciÃ³n de tareas programadas:

```
python -m crypto_app.daily_fetch --setup
```

#### Procesamiento en lote para un rango de fechas

Para procesar un rango de fechas de forma secuencial:

```
python -m crypto_app.cli bulk-process --coin bitcoin --start-date 2023-01-01 --end-date 2023-01-31
```

Para procesamiento concurrente (mÃ¡s rÃ¡pido):

```
python -m crypto_app.cli bulk-process --coin bitcoin --start-date 2023-01-01 --end-date 2023-01-31 --concurrent
```

Para almacenar tambiÃ©n en la base de datos:

```
python -m crypto_app.cli bulk-process --coin bitcoin --start-date 2023-01-01 --end-date 2023-01-31 --store-db
```
Si corriste antes una fecha pero no la almacenaste en la base de datos, no hace falta volver a correrla. PodÃ©s ejecutar el cÃ³digo 
```bash
   python load_data.py
   ```
Este script busca todos los archivos JSON en la carpeta `data/` y los carga en la tabla `coin_history`.
   

## Ejemplos de EjecuciÃ³n con Docker

Para ejecutar comandos especÃ­ficos usando Docker:

```
docker-compose run app get-history --coin bitcoin --date 2025-01-01 --store-db
```

Para el procesamiento en lote con Docker:

```
docker-compose run app bulk-process --coin bitcoin --start-date 2025-01-01 --end-date 2025-05-13 --store-db

docker-compose run app bulk-process --coin ethereum --start-date 2025-01-01 --end-date 2025-05-13 --store-db

docker-compose run app bulk-process --coin cardano --start-date 2025-01-01 --end-date 2025-05-13 --store-db
```
AclaraciÃ³n: debido a que esta es la versiÃ³n free de la api, muchas veces se alcanza el rate limit. Tuve que reejecutar varias veces algunas fechas especificas para obtener todos los datos.

Los archivos json se encuentran en la carpeta /data.

## Variables de Entorno

Crea un archivo `.env` con las siguientes variables:

```
DATABASE_URL=postgresql://username:password@localhost:5432/crypto
COINGECKO_API_KEY=tu_api_key_si_estÃ¡_disponible
```

## 2. Loading data into the database

### ğŸ› ï¸ CreaciÃ³n de tablas en PostgreSQL

Para inicializar la base de datos con las tablas necesarias (coin_history y coin_monthly_aggregates), sigue estos pasos:

1. **Asegurate de estar en el directorio del proyecto**
   ```bash
   cd crypto-data-pipeline
   ```

2. **Verificar que el archivo SQL existe**
   ```bash
   dir sql/create_tables.sql
   ```

3. **Copiar el archivo al contenedor de PostgreSQL**
   ```bash
   docker cp sql/create_tables.sql crypto-data-pipeline-db-1:/create_tables.sql
   ```
   AsegÃºrate de reemplazar el nombre del contenedor si usas uno distinto.

4. **Ejecutar el script SQL en la base de datos**
   ```bash
   docker exec -it crypto-data-pipeline-db-1 psql -U postgres -d postgres -f /create_tables.sql
   ```
   DeberÃ­as ver mensajes como CREATE TABLE, CREATE INDEX, y COMMENT, indicando que las tablas fueron creadas exitosamente.

5. **Verificar que las tablas fueron creadas**
   ```bash
   docker exec -it crypto-data-pipeline-db-1 psql -U postgres -d postgres
   ```
   Y dentro del shell de PostgreSQL, ejecuta:
   ```sql
   \dt
   ```
   DeberÃ­as ver:

| Schema | Name | Type | Owner |
|--------|------|------|-------|
| public | coin_history | table | postgres |
| public | coin_monthly_aggregates | table | postgres |

### E3. Analysing coin data with SQL ğŸ‘“

Para responder a las preguntas de la SecciÃ³n 3, sigue estos pasos:

1. **Conectate a la base de datos PostgreSQL**
   ```bash
   docker-compose exec db psql -U postgres -d postgres
   ```

2. **EjecutÃ¡ las consultas SQL desde el archivo `analysis_queries.sql`**
   
   El archivo `analysis_queries.sql` contiene dos consultas principales para responder a las preguntas de la SecciÃ³n 3:
   
   - **Query 1**: Calcula el precio promedio por mes para cada moneda
   - **Query 2**: Calcula el aumento despuÃ©s de caÃ­das consecutivas de mÃ¡s de 3 dÃ­as

   PodÃ©s copiar y pegar estas consultas desde el archivo, o ejecutar el archivo completo con:

   ```sql
   \i /docker-entrypoint-initdb.d/analysis_queries.sql
   ```

ConsultÃ¡ el archivo `sql/analysis_queries.sql` para ver la documentaciÃ³n detallada de las consultas.



```

Sigue las instrucciones para agregar la entrada necesaria a tu crontab.

## 4. Finance Meets Data Science

Navegar al directorio del proyecto e iniciar los contenedores

```
cd c:/Users/corebi/MLE Mutt/crypto-data-pipeline
docker-compose up -d
```

EjecutÃ¡ las notebooks de anÃ¡lisis exploratorio de datos y de predicciÃ³n de precios